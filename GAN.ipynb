{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlFzYAuYl29ZKhSwrekWba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KEMAL-MUDIE/Computer-Vision/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "class MnistDataset(Dataset):\n",
        "    r\"\"\"\n",
        "    Nothing special here. Just a simple dataset class for mnist images.\n",
        "    Created a dataset class rather using torchvision to allow\n",
        "    replacement with any other image dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, split, im_path, im_ext='png'):\n",
        "        r\"\"\"\n",
        "        Init method for initializing the dataset properties\n",
        "        :param split: train/test to locate the image files\n",
        "        :param im_path: root folder of images\n",
        "        :param im_ext: image extension. assumes all\n",
        "        images would be this type.\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        self.im_ext = im_ext\n",
        "        self.images, self.labels = self.load_images(im_path)\n",
        "\n",
        "    def load_images(self, im_path):\n",
        "        r\"\"\"\n",
        "        Gets all images from the path specified\n",
        "        and stacks them all up\n",
        "        :param im_path:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
        "        ims = []\n",
        "        labels = []\n",
        "        for d_name in tqdm(os.listdir(im_path)):\n",
        "            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n",
        "                ims.append(fname)\n",
        "                labels.append(int(d_name))\n",
        "        print('Found {} images for split {}'.format(len(ims), self.split))\n",
        "        return ims, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im = Image.open(self.images[index])\n",
        "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
        "        im.close()\n",
        "\n",
        "        # Uncomment below 4 lines for colored mnist images\n",
        "        # a = (im_tensor[0]*random.uniform(0.2, 1.0)).unsqueeze(0)\n",
        "        # b = (im_tensor[0]*random.uniform(0.2, 1.0)).unsqueeze(0)\n",
        "        # c = (im_tensor[0]*random.uniform(0.2, 1.0)).unsqueeze(0)\n",
        "        # im_tensor = torch.cat([a, b, c], dim=0)\n",
        "\n",
        "        # Convert input to -1 to 1 range.\n",
        "        im_tensor = (2 * im_tensor) - 1\n",
        "        return im_tensor"
      ],
      "metadata": {
        "id": "OmyU3wO0iEk0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yrxuQHAxhuOe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "# from dataset.mnist_dataset import MnistDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Configurations used for creating\n",
        "# and training GAN\n",
        "LATENT_DIM = 64\n",
        "# For colored mnist change below to 3\n",
        "IM_CHANNELS = 1\n",
        "IM_PATH = 'data/train/images'\n",
        "IM_EXT = 'png'\n",
        "IM_SIZE = (28, 28)\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 50\n",
        "NUM_SAMPLES = 225\n",
        "NROWS = 15\n",
        "##################\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    r\"\"\"\n",
        "    Generator for this gan is list of layers where each layer has the following:\n",
        "    1. Linear Layer\n",
        "    2. BatchNorm\n",
        "    3. Activation(Tanh for last layer else LeakyRELU)\n",
        "    The linear layers progressively increase dimension\n",
        "    from LATENT_DIM to IMG_H*IMG_W*IMG_CHANNELS\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.latent_dim = LATENT_DIM\n",
        "        self.img_size = IM_SIZE\n",
        "        self.channels = IM_CHANNELS\n",
        "        activation = nn.LeakyReLU()\n",
        "        layers_dim = [self.latent_dim, 128, 256, 512, self.img_size[0] * self.img_size[1] * self.channels]\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(layers_dim[i], layers_dim[i + 1]),\n",
        "                nn.BatchNorm1d(layers_dim[i + 1]) if i != len(layers_dim) - 2 else nn.Identity(),\n",
        "                activation if i != len(layers_dim) - 2 else nn.Tanh()\n",
        "            )\n",
        "            for i in range(len(layers_dim) - 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, z):\n",
        "        batch_size = z.shape[0]\n",
        "        out = z.reshape(-1, self.latent_dim)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "        out = out.reshape(batch_size, self.channels, self.img_size[0], self.img_size[1])\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    r\"\"\"\n",
        "    Discriminator mimicks the design of generator\n",
        "    only reduces dimensions progressive rather than increasing.\n",
        "    From IMG_H*IMG_W*IMG_CHANNELS it reduces all the way to 1 where\n",
        "    the last value is the probability discriminator thinks that\n",
        "    given image is real(closer to 1 if real else closer to 0)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.img_size = IM_SIZE\n",
        "        self.channels = IM_CHANNELS\n",
        "        activation = nn.LeakyReLU()\n",
        "        layers_dim = [self.img_size[0] * self.img_size[1] * self.channels, 512, 256, 128, 1]\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(layers_dim[i], layers_dim[i + 1]),\n",
        "                nn.LayerNorm(layers_dim[i + 1]) if i != len(layers_dim) - 2 else nn.Identity(),\n",
        "                activation if i != len(layers_dim) - 2 else nn.Identity()\n",
        "            )\n",
        "            for i in range(len(layers_dim) - 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x.reshape(-1, self.img_size[0] * self.img_size[1] * self.channels)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Create the dataset\n",
        "    mnist = MnistDataset('train', im_path=IM_PATH, im_ext=IM_EXT)\n",
        "    mnist_loader = DataLoader(mnist, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Instantiate the model\n",
        "    generator = Generator().to(device)\n",
        "    discriminator = Discriminator().to(device)\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # Specify training parameters\n",
        "    optimizer_generator = Adam(generator.parameters(), lr=1E-4, betas=(0.5, 0.999))\n",
        "    optimizer_discriminator = Adam(discriminator.parameters(), lr=1E-4, betas=(0.5, 0.999))\n",
        "\n",
        "    # Criterion is bcewithlogits hence no sigmoid in discriminator\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Run training\n",
        "    steps = 0\n",
        "    generated_sample_count = 0\n",
        "    for epoch_idx in range(NUM_EPOCHS):\n",
        "        generator_losses = []\n",
        "        discriminator_losses = []\n",
        "        mean_real_dis_preds = []\n",
        "        mean_fake_dis_preds = []\n",
        "        for im in tqdm(mnist_loader):\n",
        "            real_ims = im.float().to(device)\n",
        "            batch_size = real_ims.shape[0]\n",
        "\n",
        "            # Optimize Discriminator\n",
        "            optimizer_discriminator.zero_grad()\n",
        "            fake_im_noise = torch.randn((batch_size, LATENT_DIM), device=device)\n",
        "            fake_ims = generator(fake_im_noise)\n",
        "            real_label = torch.ones((batch_size, 1), device=device)\n",
        "            fake_label = torch.zeros((batch_size, 1), device=device)\n",
        "\n",
        "            disc_real_pred = discriminator(real_ims)\n",
        "            disc_fake_pred = discriminator(fake_ims.detach())\n",
        "            disc_real_loss = criterion(disc_real_pred.reshape(-1), real_label.reshape(-1))\n",
        "            mean_real_dis_preds.append(torch.nn.Sigmoid()(disc_real_pred).mean().item())\n",
        "\n",
        "            disc_fake_loss = criterion(disc_fake_pred.reshape(-1), fake_label.reshape(-1))\n",
        "            mean_fake_dis_preds.append(torch.nn.Sigmoid()(disc_fake_pred).mean().item())\n",
        "            disc_loss = (disc_real_loss + disc_fake_loss) / 2\n",
        "            disc_loss.backward()\n",
        "            optimizer_discriminator.step()\n",
        "            ########################\n",
        "\n",
        "            # Optimize Generator\n",
        "            optimizer_generator.zero_grad()\n",
        "            fake_im_noise = torch.randn((batch_size, LATENT_DIM), device=device)\n",
        "            fake_ims = generator(fake_im_noise)\n",
        "            disc_fake_pred = discriminator(fake_ims)\n",
        "            gen_fake_loss = criterion(disc_fake_pred.reshape(-1), real_label.reshape(-1))\n",
        "            gen_fake_loss.backward()\n",
        "            optimizer_generator.step()\n",
        "            ########################\n",
        "\n",
        "            generator_losses.append(gen_fake_loss.item())\n",
        "            discriminator_losses.append(disc_loss.item())\n",
        "\n",
        "            # Save samples\n",
        "            if steps % 50 == 0:\n",
        "                with torch.no_grad():\n",
        "                    generator.eval()\n",
        "                    infer(generated_sample_count, generator)\n",
        "                    generated_sample_count += 1\n",
        "                    generator.train()\n",
        "            #############\n",
        "            steps += 1\n",
        "        print('Finished epoch:{} | Generator Loss : {:.4f} | Discriminator Loss : {:.4f} | '\n",
        "              'Discriminator real pred : {:.4f} | Discriminator fake pred : {:.4f}'.format(\n",
        "            epoch_idx + 1,\n",
        "            np.mean(generator_losses),\n",
        "            np.mean(discriminator_losses),\n",
        "            np.mean(mean_real_dis_preds),\n",
        "            np.mean(mean_fake_dis_preds),\n",
        "        ))\n",
        "        torch.save(generator.state_dict(), 'generator_ckpt.pth')\n",
        "        torch.save(discriminator.state_dict(), 'discriminator_ckpt.pth')\n",
        "\n",
        "    print('Done Training ...')\n",
        "\n",
        "\n",
        "def infer(generated_sample_count, generator):\n",
        "    r\"\"\"\n",
        "    Method to save the generated samples\n",
        "    :param generated_sample_count: Filename to save the output with\n",
        "    :param generator: Generator model with trained parameters\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    fake_im_noise = torch.randn((NUM_SAMPLES, LATENT_DIM), device=device)\n",
        "    fake_ims = generator(fake_im_noise)\n",
        "    ims = torch.clamp(fake_ims, -1., 1.).detach().cpu()\n",
        "    ims = (ims + 1) / 2\n",
        "    grid = make_grid(ims, nrow=NROWS)\n",
        "    img = torchvision.transforms.ToPILImage()(grid)\n",
        "    if not os.path.exists('samples'):\n",
        "        os.mkdir('samples')\n",
        "    img.save('samples/{}.png'.format(generated_sample_count))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "GzOGH2g5hzHp",
        "outputId": "a33340f1-8343-4d4b-fa7f-67ed1d1cc7c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "images path data/train/images does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2da0ffaf5447>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-1b945277e596>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Create the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMnistDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIM_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_ext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIM_EXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mmnist_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-41d2ff3bcbeb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, im_path, im_ext)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_ext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-41d2ff3bcbeb>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(self, im_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \"\"\"\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"images path {} does not exist\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: images path data/train/images does not exist"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7H1kCcCiODJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}